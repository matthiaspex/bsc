 experiment:
   interface: 'HPC' # HPC or VSC 
   run_name_format: "{notes} {reward_type} {arm_setup} popsize {es_popsize} {cost_expr} {efficiency_expr} hebbian {hebbian}"
   seed: 2024
 wandb:
   project: "job_sub_test"
   group: "Donphan"
   tags: [ "hebbian", "distance", "cost_efficiency" ]
   notes: "b11_donphan_linux"
 morphology:
   arm_setup: [ 5, 5, 5, 5, 5 ]
   joint_control: 'position'
 arena:
   arena_size: !!python/tuple [ 10, 5 ]
   sand_ground_color: False
 environment:
   reward_type: 'distance' # choose from distance, target or light
   sensor_selection: !!python/tuple [ 'joint_position', 'joint_actuator_force', 'segment_contact']
   num_physics_steps_per_control_step: 10
   simulation_time: 10 # use 10 for Hebbian and 5 for static
   joint_randomization_noise_scale: 0.0
   light_perlin_noise_scale: 4
   target_distance: 9
   target_position: [ 9., 0., 0.] # must fit in the arena
   color_contacts: False
   render:
     camera_ids: [ 0, 1 ]
     render_size: [ 480, 640 ] # choose ratio 3:4 --> [ 480, 640  ], [ 720, 960 ], [ 960, 1280 ] (720p), [ 1440, 1920 ] (1080p), [ 3072, 4069 ] (HDTV 4k)
 damage:
   damage: True # try to write code indepent of this argument
   arm_setup_damage: [ 5, 0, 5, 5, 5]
 evolution:
   es_popsize: 6912
   num_generations: 100
   cost_expr: "torque" # choose from ["nocost", "torque x angvel", "torque"]
   penal_expr: "nopenal" # choose from ["nopenal", "body_stability"]
   efficiency_expr: "reward _ cost" # choose from ["reward", "reward _ cost", "reward + reward _ cost"]
 controller:
   hebbian: True
   hidden_layers: [ 128, 128 ]
 training:
   target:
     distance: 7
     num_rowing: 1 # max 5, only 5 different rowing positions possible
     num_reverse_rowing: 0 # max 5, only 5 different rowing positions possible
     num_random_positions: 0
     # total number of targets is the sum of targets in rowing, reverse_rowing and random positions
     parallel_constant: False # whether all parallel environments during training get the same targets
