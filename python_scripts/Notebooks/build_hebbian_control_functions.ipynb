{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to build the Hebbian Learning module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Matthias\\OneDrive - UGent\\Documents\\DOCUMENTEN\\3. Thesis\\BSC\\bsc\\Scripts\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "sys.path.insert(0,'C:\\\\Users\\\\Matthias\\\\OneDrive - UGent\\\\Documents\\\\DOCUMENTEN\\\\3. Thesis\\\\BSC\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chex\n",
    "from typing import Sequence\n",
    "import copy\n",
    "\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "from evosax import ParameterReshaper, OpenES\n",
    "\n",
    "from bsc_utils.controller.base import ExplicitMLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(0)\n",
    "\n",
    "jnp.set_printoptions(precision = 15, suppress = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.1378784  -1.2209548  -0.59153634]\n"
     ]
    }
   ],
   "source": [
    "features = [4,5,2] # 2 hidden layers of 4 and 5 neurons, output dim is 2. Input dim is defined by the input vector used for instantiation\n",
    "rng, rng_input = jax.random.split(rng, 2)\n",
    "input = jax.random.normal(rng_input, (3,))\n",
    "print(input)\n",
    "joint_control = 'position'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ExplicitMLP(features = features, joint_control = joint_control) # uses the ExplicitMLP I have defined before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExplicitMLP(\n",
      "    # attributes\n",
      "    features = [4, 5, 2]\n",
      "    joint_control = 'position'\n",
      ")\n",
      "{'params': {'layers_0': {'kernel': Array([[ 0.91562444 , -0.043203816,  1.06029    ,  1.0857037  ],\n",
      "       [-0.015480848,  1.1917892  , -0.6570196  ,  1.1079454  ],\n",
      "       [ 0.2895664  , -0.24119082 ,  1.0076706  , -0.9029162  ]],      dtype=float32), 'bias': Array([0., 0., 0., 0.], dtype=float32)}, 'layers_1': {'kernel': Array([[ 0.42656583 , -0.0712561  ,  0.23940565 , -0.30804867 ,\n",
      "        -0.780432   ],\n",
      "       [-0.0604636  ,  0.54123    ,  0.78129745 ,  0.02796052 ,\n",
      "        -0.013180583],\n",
      "       [-0.3252826  ,  0.55973214 ,  0.16509312 ,  0.2372554  ,\n",
      "        -0.43057743 ],\n",
      "       [ 0.10360184 ,  0.7279873  ,  0.2179734  ,  0.07189767 ,\n",
      "        -0.97823316 ]], dtype=float32), 'bias': Array([0., 0., 0., 0., 0.], dtype=float32)}, 'layers_2': {'kernel': Array([[ 0.71163124, -0.5308686 ],\n",
      "       [-0.10842434,  0.75031364],\n",
      "       [ 0.3031431 , -0.297253  ],\n",
      "       [ 1.0005071 , -0.2076626 ],\n",
      "       [ 0.67807704, -0.4651766 ]], dtype=float32), 'bias': Array([0., 0.], dtype=float32)}}}\n",
      "{'params': {'layers_0': {'bias': (4,), 'kernel': (3, 4)}, 'layers_1': {'bias': (5,), 'kernel': (4, 5)}, 'layers_2': {'bias': (2,), 'kernel': (5, 2)}}}\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "rng, rng_init = jax.random.split(rng, 2)\n",
    "params_init = model.init(rng_init, input)\n",
    "print(params_init)\n",
    "print(jax.tree_util.tree_map(lambda x: x.shape, params_init)) # tree_map treats np.arrays as a single leaf, whereas lists, tuples, dicts are nodes (nodes are in itself pytrees)\n",
    "# the leaves of a tuple/list are the elements of that tuple/list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.29033384  0.28768677]\n",
      "[Array([ 1.1378784 , -1.2209548 , -0.59153634], dtype=float32), Array([ 0.71113765, -0.87676555,  0.8880447 ,  0.39419347], dtype=float32), Array([ 0.10791126,  0.25320113, -0.27496824, -0.00454481, -0.86463517],      dtype=float32), Array([-0.29033384,  0.28768677], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# model apply now provides neuron_activities and the actual provided action seperately\n",
    "\n",
    "x, neuron_activities = model.apply(params_init, input)\n",
    "print(x)\n",
    "print(neuron_activities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 1, 2)\n"
     ]
    }
   ],
   "source": [
    "tup = (0,1)\n",
    "tup_ext = tuple(list(tup)+[2])\n",
    "print(tup_ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_learning_rule_dim(x, n=5):\n",
    "    \"\"\"\n",
    "    Apply on pytrees containtining 2D kernals and 1D bias arrays\n",
    "    Adds a learning rule of n parameters to each kernel in an additional dimension\n",
    "    Only applies this to kernels, not to bias arrays\n",
    "    each kernel of shape (i,j) becomes a kernel of shape (i,j,n)\n",
    "    each bias of shape (m,) remains bias of shape (m,)\n",
    "    RETURNED ARRAY CONTAINS ZEROES\n",
    "    \"\"\"\n",
    "    if len(x.shape) == 2:\n",
    "        new_shape = tuple(list(x.shape)+[n])\n",
    "        return jnp.zeros(new_shape)\n",
    "    else:\n",
    "        return jnp.zeros_like(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "[ 7  8  9 10]\n",
      "[[[0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]]]\n",
      "[0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "x = jnp.array([[1,2,3],[4,5,6]])\n",
    "y = jnp.array([7,8,9,10])\n",
    "\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "print(add_learning_rule_dim(x))\n",
    "print(add_learning_rule_dim(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'params': {'layers_0': {'bias': (4,), 'kernel': (3, 4, 5)}, 'layers_1': {'bias': (5,), 'kernel': (4, 5, 5)}, 'layers_2': {'bias': (2,), 'kernel': (5, 2, 5)}}}\n",
      "{'bias': Array([0., 0., 0., 0., 0.], dtype=float32), 'kernel': Array([[[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]]], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "learning_rules_empty = jax.tree_util.tree_map(lambda x: add_learning_rule_dim(x,n=5), params_init)\n",
    "print(jax.tree_util.tree_map(lambda x: x.shape, learning_rules_empty))\n",
    "print(learning_rules_empty[\"params\"][\"layers_1\"])\n",
    "                                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParameterReshaper: 221 parameters detected for optimization.\n",
      "num_params in learning rule tree: 221\n",
      "ParameterReshaper: 53 parameters detected for optimization.\n",
      "num_params in synapse strength tree: 53\n",
      "ratio of params increase: 4.169811320754717\n"
     ]
    }
   ],
   "source": [
    "param_reshaper = ParameterReshaper(learning_rules_empty)\n",
    "num_params = param_reshaper.total_params\n",
    "print(f\"num_params in learning rule tree: {num_params}\")\n",
    "# to compare)\n",
    "param_reshaper_synapse_strength = ParameterReshaper(params_init)\n",
    "num_params_synapse_strengths = param_reshaper_synapse_strength.total_params\n",
    "print(f\"num_params in synapse strength tree: {num_params_synapse_strengths}\")\n",
    "print(f\"ratio of params increase: {num_params/num_params_synapse_strengths}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "candidate solution shape: (2, 221)\n",
      "\n",
      "      reshaped policy params in jax:\n",
      "      {'params': {'layers_0': {'bias': (2, 4), 'kernel': (2, 3, 4, 5)}, 'layers_1': {'bias': (2, 5), 'kernel': (2, 4, 5, 5)}, 'layers_2': {'bias': (2, 2), 'kernel': (2, 5, 2, 5)}}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get learning rules from evosax\n",
    "popsize = 2\n",
    "\n",
    "rng, rng_ask, rng_init = jax.random.split(rng, 3)\n",
    "strategy  = OpenES(popsize = popsize, num_dims = num_params)\n",
    "es_params = strategy.default_params\n",
    "\n",
    "es_state = strategy.initialize(rng_init, es_params)\n",
    "learning_rules_flat, es_state = strategy.ask(rng_ask, es_state)\n",
    "print(f\"candidate solution shape: {learning_rules_flat.shape}\")\n",
    "\n",
    "learning_rules = param_reshaper.reshape(learning_rules_flat)\n",
    "print(f\"\"\"\n",
    "      reshaped policy params in jax:\n",
    "      {jax.tree_util.tree_map(lambda x: x.shape, learning_rules)}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synaptic strength initialisation: (2, 53)\n",
      "\n",
      "      reshaped policy params in jax:\n",
      "      {'params': {'layers_0': {'bias': (2, 4), 'kernel': (2, 3, 4)}, 'layers_1': {'bias': (2, 5), 'kernel': (2, 4, 5)}, 'layers_2': {'bias': (2, 2), 'kernel': (2, 5, 2)}}}\n",
      "\n",
      "{'params': {'layers_0': {'bias': Array([[ 0.08441029  , -0.07492232  , -0.05319176  , -0.03989966  ],\n",
      "       [ 0.0048117638,  0.051953316 , -0.09334455  , -0.09554732  ]],      dtype=float32), 'kernel': Array([[[ 0.06805022  ,  0.023456668 , -0.033478856 ,  0.08947175  ],\n",
      "        [ 0.07558513  , -0.027463222 ,  0.04407022  , -0.03388474  ],\n",
      "        [ 0.046473026 , -0.097225785 ,  0.049788833 , -0.071477346 ]],\n",
      "\n",
      "       [[ 0.07617481  , -0.090289235 ,  0.08835657  ,  0.07394917  ],\n",
      "        [-0.0667928   ,  0.07353234  , -0.07089801  ,  0.07646527  ],\n",
      "        [-0.0061877253,  0.041908145 , -0.08476386  , -0.026584268 ]]],      dtype=float32)}, 'layers_1': {'bias': Array([[-0.046349265, -0.012251282, -0.09816499 ,  0.049443055,\n",
      "         0.03825426 ],\n",
      "       [-0.053711962, -0.03270197 , -0.09794777 ,  0.014555908,\n",
      "         0.064900376]], dtype=float32), 'kernel': Array([[[ 0.015831495 , -0.043354463 ,  0.018450094 , -0.056299496 ,\n",
      "         -0.075327635 ],\n",
      "        [-0.07377267  , -0.010723329 , -0.0014111757,  0.008242631 ,\n",
      "         -0.07530417  ],\n",
      "        [-0.08754573  ,  0.09358056  ,  0.097400285 , -0.016722728 ,\n",
      "         -0.061801743 ],\n",
      "        [-0.025607849 , -0.03324938  ,  0.037265133 , -0.057018947 ,\n",
      "         -0.031114865 ]],\n",
      "\n",
      "       [[-0.051102664 , -0.040912032 , -0.062057115 , -0.016464425 ,\n",
      "          0.019795561 ],\n",
      "        [ 0.033943105 ,  0.025718523 ,  0.076464586 ,  0.032807518 ,\n",
      "         -0.098408535 ],\n",
      "        [ 0.059708692 , -0.011993456 ,  0.017661143 , -0.035028983 ,\n",
      "         -0.014089084 ],\n",
      "        [ 0.09789057  , -0.07015922  , -0.053637337 ,  0.09443698  ,\n",
      "          0.016882468 ]]], dtype=float32)}, 'layers_2': {'bias': Array([[-0.080723576, -0.049667716],\n",
      "       [-0.050449442, -0.028392578]], dtype=float32), 'kernel': Array([[[ 0.061501194 ,  0.0032880306],\n",
      "        [ 0.045695495 ,  0.023116875 ],\n",
      "        [-0.09161549  ,  0.038179018 ],\n",
      "        [ 0.069737054 ,  0.096010685 ],\n",
      "        [ 0.022559715 ,  0.06425264  ]],\n",
      "\n",
      "       [[-0.07025113  , -0.089499615 ],\n",
      "        [ 0.0796865   , -0.064158514 ],\n",
      "        [-0.059112646 ,  0.020290947 ],\n",
      "        [-0.063518934 , -0.07856579  ],\n",
      "        [-0.0726676   ,  0.010380483 ]]], dtype=float32)}}}\n"
     ]
    }
   ],
   "source": [
    "rng, rng_unif = jax.random.split(rng, 2)\n",
    "synapse_strengths_init_flat = jax.random.uniform(rng_unif, shape=(popsize, num_params_synapse_strengths), minval = -0.1, maxval = 0.1) # based on Pedersen & Risi (2021)\n",
    "print(f\"synaptic strength initialisation: {synapse_strengths_init_flat.shape}\")\n",
    "\n",
    "synapse_strengths_init = param_reshaper_synapse_strength.reshape(synapse_strengths_init_flat)\n",
    "print(f\"\"\"\n",
    "      reshaped policy params in jax:\n",
    "      {jax.tree_util.tree_map(lambda x: x.shape, synapse_strengths_init)}\n",
    "\"\"\")\n",
    "print(synapse_strengths_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Array([[ 1.1378784 , -1.2209548 , -0.59153634],\n",
      "       [ 1.1378784 , -1.2209548 , -0.59153634]], dtype=float32), Array([[0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0.]], dtype=float32), Array([[0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0.]], dtype=float32), Array([[0., 0.],\n",
      "       [0., 0.]], dtype=float32)]\n",
      "[(2, 3), (2, 4), (2, 5), (2, 2)]\n"
     ]
    }
   ],
   "source": [
    "input_stack = jnp.tile(input, (popsize, 1))\n",
    "neuron_activities = [jnp.zeros((2, n)) for n in features]\n",
    "neuron_activities = [input_stack] + neuron_activities\n",
    "print(neuron_activities)\n",
    "print(jax.tree_util.tree_map(lambda x: x.shape, neuron_activities))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_learning_rule(lr_kernel, input_nodes, output_nodes):\n",
    "    \"\"\" \n",
    "    Input: learning rule kernel: dims (popsize, input_layer_dim, output_layer_dim, lr_dim = 5)\n",
    "    Output: synaptic strength increment kernel: dims (popsize, input_layer_dim, output_layer_dim)\n",
    "    This function is vmapable and jittable\n",
    "    \"\"\"\n",
    "    assert lr_kernel.shape[-1] == 5, \"Learning rule requires 5 parameters or needs to be updated so it is compatible with different number of parameters\"\n",
    "    in_dim = len(input_nodes)\n",
    "    out_dim = len(output_nodes)\n",
    "    inp = jnp.transpose(jnp.tile(input_nodes, (out_dim, 1))) # Generates (in_dim, out_dim) dimension, but constant along axis = 1 (output dimesnion)\n",
    "    outp = jnp.tile(output_nodes, (in_dim, 1)) # Generates (in_dim, out_dim) dimension, but constant along axis = 0 (input dimesnion)\n",
    "\n",
    "    # kernel content: [alpha, A, B, C, D] --> Dw_ij = alpha_ij * (A_ij*o_i*o_j + B_ij * o_i + C_ij * o_j + D_ij)\n",
    "    alpha = lr_kernel[:,:,0]\n",
    "    A = lr_kernel[:,:,1]\n",
    "    B = lr_kernel[:,:,2]\n",
    "    C = lr_kernel[:,:,3]\n",
    "    D = lr_kernel[:,:,4]\n",
    "    ss_incr_kernel = alpha * (A*inp*outp + B*inp + C*outp + D)\n",
    "\n",
    "    return ss_incr_kernel # synaptic strength increment kernel\n",
    "\n",
    "apply_learning_rule_vect = jax.jit(jax.vmap(apply_learning_rule))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "synapse_strengths_init_save = copy.deepcopy(synapse_strengths_init)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'params': {'layers_0': {'bias': (2, 4), 'kernel': (2, 3, 4)}, 'layers_1': {'bias': (2, 5), 'kernel': (2, 4, 5)}, 'layers_2': {'bias': (2, 2), 'kernel': (2, 5, 2)}}}\n",
      "{'params': {'layers_0': {'bias': Array([[ 0.095749214 , -0.08305403  , -0.051500898 , -0.027483817 ],\n",
      "       [-0.0065271594,  0.06008502  , -0.09503541  , -0.10796316  ]],      dtype=float32), 'kernel': Array([[[ 0.0003116727 ,  0.001973167  ,  0.0034631677 , -0.023116484  ],\n",
      "        [ 0.00055105984,  0.0043627936 ,  0.016441975  ,  0.033707418  ],\n",
      "        [-0.0042980723 , -0.015313536  ,  0.0009611361 , -0.008618005  ]],\n",
      "\n",
      "       [[ 0.0003116727 ,  0.001973167  ,  0.0034631342 , -0.023116484  ],\n",
      "        [ 0.00055105984,  0.004362814  ,  0.016441941  ,  0.03370741   ],\n",
      "        [-0.004298064  , -0.015313536  ,  0.0009611696 , -0.008617988  ]]],      dtype=float32)}, 'layers_1': {'bias': Array([[-0.07035379  , -0.039831672 , -0.07514771  , -0.007336233 ,\n",
      "         0.031622782 ],\n",
      "       [-0.029707434 , -0.0051215794, -0.12096505  ,  0.0713352   ,\n",
      "         0.071531855 ]], dtype=float32), 'kernel': Array([[[ 1.11097936e-02, -1.29282475e-04, -6.20071404e-03,\n",
      "         -1.44906342e-04,  5.25444746e-04],\n",
      "        [ 2.24775821e-03,  1.52303847e-02, -6.45149848e-04,\n",
      "          2.03747302e-04, -5.69298863e-05],\n",
      "        [-1.60235167e-03, -6.07071072e-03, -2.48584449e-02,\n",
      "         -2.10486818e-03,  4.10163030e-03],\n",
      "        [ 6.24151714e-03,  7.69358128e-04, -3.52241099e-04,\n",
      "          8.54283571e-04, -2.30718590e-03]],\n",
      "\n",
      "       [[ 1.11097805e-02, -1.29282475e-04, -6.20073080e-03,\n",
      "         -1.44923106e-04,  5.25427982e-04],\n",
      "        [ 2.24772468e-03,  1.52303837e-02, -6.45138323e-04,\n",
      "          2.03747302e-04, -5.69298863e-05],\n",
      "        [-1.60235167e-03, -6.07073586e-03, -2.48584114e-02,\n",
      "         -2.10486352e-03,  4.10161447e-03],\n",
      "        [ 6.24150038e-03,  7.69324601e-04, -3.52241099e-04,\n",
      "          8.54283571e-04, -2.30718590e-03]]], dtype=float32)}, 'layers_2': {'bias': Array([[-0.122686386, -0.0696336  ],\n",
      "       [-0.008486636, -0.008426687]], dtype=float32), 'kernel': Array([[[ 0.00082937256, -0.0026640743 ],\n",
      "        [ 0.0022212379 ,  0.0027614683 ],\n",
      "        [-0.00032474846,  0.00042878464],\n",
      "        [-0.027522951  , -0.0040149987 ],\n",
      "        [-0.0011693425 ,  0.0046980903 ]],\n",
      "\n",
      "       [[ 0.000829339  , -0.002664104  ],\n",
      "        [ 0.0022212043 ,  0.0027614683 ],\n",
      "        [-0.00032474846,  0.0004288014 ],\n",
      "        [-0.027522951  , -0.0040149987 ],\n",
      "        [-0.0011693761 ,  0.0046981047 ]]], dtype=float32)}}}\n"
     ]
    }
   ],
   "source": [
    "def update_synapse_strengths(\n",
    "        synapse_strengths_input: dict,\n",
    "        learning_rules: dict, # pytree\n",
    "        neuron_activities: Sequence[chex.Array] # jax array\n",
    "        ) -> dict:\n",
    "    # problem: for some reason adjusting the dicts in this function causes the global value to be affected. For security, use copy.deepcopy\n",
    "    synapse_strengths = copy.deepcopy(synapse_strengths_input)\n",
    "\n",
    "    num_layers = len(learning_rules[\"params\"].keys())\n",
    "    for p in range(num_layers):\n",
    "        lr_kernel = learning_rules[\"params\"][f\"layers_{p}\"][\"kernel\"]\n",
    "        input_nodes = neuron_activities[p]\n",
    "        output_nodes = neuron_activities[p+1]\n",
    "\n",
    "        ss_incr_kernel = apply_learning_rule_vect(lr_kernel, input_nodes, output_nodes)\n",
    "\n",
    "        synapse_strengths[\"params\"][f\"layers_{p}\"][\"kernel\"] += ss_incr_kernel\n",
    "        synapse_strengths[\"params\"][f\"layers_{p}\"][\"bias\"] = learning_rules[\"params\"][f\"layers_{p}\"][\"bias\"]\n",
    "\n",
    "    return synapse_strengths\n",
    "\n",
    "\n",
    "synapse_strengths = update_synapse_strengths(synapse_strengths_init, learning_rules, neuron_activities)\n",
    "print(jax.tree_util.tree_map(lambda x: x.shape, synapse_strengths))\n",
    "print(jax.tree_util.tree_map(lambda x,y: x-y, synapse_strengths_init_save, synapse_strengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bsc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
